{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.318865Z",
     "start_time": "2024-04-08T17:02:19.891195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in ./.local/lib/python3.11/site-packages (5.19.0)\r\n",
      "Requirement already satisfied: sentence_transformers in ./.local/lib/python3.11/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.11/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: hdbscan in /opt/conda/lib/python3.11/site-packages (0.8.33)\r\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.11/site-packages (3.7.4)\r\n",
      "Requirement already satisfied: pytz in ./.local/lib/python3.11/site-packages (from neo4j) (2024.1)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in ./.local/lib/python3.11/site-packages (from sentence_transformers) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.11/site-packages (from sentence_transformers) (4.66.2)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.local/lib/python3.11/site-packages (from sentence_transformers) (2.2.2)\r\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.11/site-packages (from sentence_transformers) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.11/site-packages (from sentence_transformers) (1.4.1.post1)\r\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.11/site-packages (from sentence_transformers) (1.13.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./.local/lib/python3.11/site-packages (from sentence_transformers) (0.22.2)\r\n",
      "Requirement already satisfied: Pillow in ./.local/lib/python3.11/site-packages (from sentence_transformers) (10.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.11/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.11/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.11/site-packages (from nltk) (2023.12.25)\r\n",
      "Requirement already satisfied: cython<3,>=0.27 in /opt/conda/lib/python3.11/site-packages (from hdbscan) (0.29.37)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.11/site-packages (from spacy) (8.2.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (0.9.4)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from spacy) (6.4.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.6.4)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.1.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy) (69.2.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (24.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.4.0)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\r\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install neo4j sentence_transformers pandas nltk hdbscan spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fb704df2cbfe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build our dataset from Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c47932ddc9b0b594",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.329665Z",
     "start_time": "2024-04-08T17:02:23.323053Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import textwrap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NEO4J_URI = \"bolt://neo4j.neo4j.svc.cluster.local\"\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE = 'neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bc8e87f9f459798",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.368270Z",
     "start_time": "2024-04-08T17:02:23.332555Z"
    }
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "auth = (\"neo4j\", \"keZSjc1CaHTakP\")\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=auth) as driver:\n",
    "    driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5a27e75fddfaeba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.438132Z",
     "start_time": "2024-04-08T17:02:23.370664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat logs saved to chat_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to execute the query and return results as a pandas DataFrame\n",
    "def get_chat_logs_as_dataframe(driver):\n",
    "    query = \"\"\"\n",
    "    MATCH (m:Message)-[:POSTED_IN]->(c:Channel), (u:User)-[:SENT]->(m)\n",
    "    OPTIONAL MATCH (m)-[:MENTIONED]->(mentioned:User)\n",
    "    RETURN u.name AS user, c.name AS channel, m.timestamp AS timestamp, m.content AS message\n",
    "    ORDER BY m.timestamp DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        results = session.run(query)\n",
    "        \n",
    "        # Convert results to a DataFrame\n",
    "        chat_logs_df = pd.DataFrame([record.data() for record in results])\n",
    "        \n",
    "        # Optionally, you can save the DataFrame to a CSV file for easy use\n",
    "        chat_logs_df.to_csv(\"chat_logs.csv\", index=False)\n",
    "        \n",
    "        print(\"Chat logs saved to chat_logs.csv\")\n",
    "        \n",
    "        return chat_logs_df\n",
    "\n",
    "# Call the function to get chat logs as a pandas DataFrame\n",
    "chat_logs_df = get_chat_logs_as_dataframe(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc96315c51ea7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Break the chat logs into conversational contexts\n",
    "\n",
    "## Possible Approaches:\n",
    "\n",
    "1. **Clustering**: Use Clustering algorithms to group the chat logs into conversational contexts,\n",
    "like K-Means or DBSCAN. We'll use the `message` column as the feature to cluster on.\n",
    "2. **Time-based**: Group chat logs based on a time window, like every 5 minutes.\n",
    "3. **User-based**: Group chat logs based on the user who sent the message.\n",
    "4. **Channel-based**: Group chat logs based on the channel where the message was posted.\n",
    "5. **Sequential**: Group chat logs based on the order they were posted.\n",
    "6. **Sequence Labeling**: Use Sequence Labeling models to predict the start and end of each conversation, like Named Entity Recognition (NER) models, Conditional Random Fields (CRFs), Hidden Markov Model (HMM) or Long Short-Term Memory (LSTM) networks to label each message with a conversation ID. To do this, we need to train our model on labeled data to learn the conversational patterns that distinguish between different conversational threads. We can use this model to label new messages automatically.\n",
    "7. **Transformer Based Methods**: Use transformer-based models like BERT, GPT-2, or RoBERTa to generate embeddings for each message and cluster them based on the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing and Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f39bf6721f324724"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5402d8fa16bbf26",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.453936Z",
     "start_time": "2024-04-08T17:02:23.442337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic text cleaning and preprocessing\n",
    "# You might want to expand this with more sophisticated cleaning\n",
    "chat_logs_df['message_clean'] = chat_logs_df['message'].str.lower().str.replace('[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove stop words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcb48559721da5e9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.463838Z",
     "start_time": "2024-04-08T17:02:23.456252Z"
    }
   },
   "id": "e2e96b2445caa154",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "chat_logs_df['message_clean'] = chat_logs_df['message_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:23.486066Z",
     "start_time": "2024-04-08T17:02:23.466321Z"
    }
   },
   "id": "5e5a26b6ac3df09d",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cd6fd3e82d750fe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m44.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([token.lemma_ for token in nlp(text)])\n",
    "\n",
    "chat_logs_df['message_clean'] = chat_logs_df['message_clean'].apply(lemmatize_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:27.768026Z",
     "start_time": "2024-04-08T17:02:23.487578Z"
    }
   },
   "id": "3eff047b0b725848",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing frequent but unimportant words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cecd0bd3855555ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build frequent_words\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the cleaned messages into lists of words\n",
    "chat_logs_df['tokens'] = chat_logs_df['message_clean'].str.split()\n",
    "\n",
    "# Flatten the list of token lists into a single list\n",
    "all_words = [word for tokens in chat_logs_df['tokens'] for word in tokens]\n",
    "\n",
    "# Count the words\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Set a frequency threshold\n",
    "frequency_threshold = 100  # This is just an example value\n",
    "\n",
    "# Filter words that meet or exceed the threshold\n",
    "frequent_words = {word for word, count in word_counts.items() if count >= frequency_threshold}\n",
    "\n",
    "chat_logs_df['message_clean'] = chat_logs_df['message_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in frequent_words]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:27.775883Z",
     "start_time": "2024-04-08T17:02:27.769054Z"
    }
   },
   "id": "8bb2b5a7ce4e8677",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing rare words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1a6179a7ea3981f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set a frequency threshold for rare words\n",
    "#rare_threshold = 2  # Example: words appearing 2 times or less\n",
    "\n",
    "# Filter words that are equal to or below the threshold\n",
    "#rare_words = {word for word, count in word_counts.items() if count <= rare_threshold}\n",
    "\n",
    "#chat_logs_df['message_clean'] = chat_logs_df['message_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in rare_words]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:27.796293Z",
     "start_time": "2024-04-08T17:02:27.777959Z"
    }
   },
   "id": "ad0acc4961e0f0c7",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "7635ad8036fa7397",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b5826dd17cc133",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:28.949803Z",
     "start_time": "2024-04-08T17:02:27.798804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0de68f763c946af97bacc05c5d40631"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully!\n",
      "[[-0.11883839  0.0482987  -0.00254814 ...  0.12640943  0.04654907\n",
      "  -0.01571724]\n",
      " [-0.13658412  0.01173701 -0.00966252 ...  0.02726141 -0.01584398\n",
      "   0.02466037]\n",
      " [-0.0427901   0.04457811  0.01399177 ...  0.06319794  0.03106361\n",
      "  -0.03714633]\n",
      " ...\n",
      " [-0.03119157  0.01449915 -0.04481908 ... -0.01519592  0.0967582\n",
      "   0.09199005]\n",
      " [-0.10109371  0.046329    0.04294018 ... -0.03402774 -0.0196047\n",
      "   0.02392813]\n",
      " [-0.07301831 -0.05540538 -0.07172503 ...  0.00799474  0.06458855\n",
      "   0.03423898]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "message_embeddings = model.encode(chat_logs_df['message_clean'].tolist(), show_progress_bar=True)\n",
    "print(\"Embeddings generated successfully!\")\n",
    "\n",
    "# verify the shape of the embeddings\n",
    "print(message_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151f514a46873f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cluster the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39577374d77b0533",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:29.036477Z",
     "start_time": "2024-04-08T17:02:28.950757Z"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "# Normalize embeddings to improve clustering\n",
    "message_embeddings_normalized = normalize(message_embeddings)\n",
    "\n",
    "# Clustering with HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "cluster_labels = clusterer.fit_predict(message_embeddings_normalized)\n",
    "\n",
    "# Add cluster labels to your DataFrame\n",
    "chat_logs_df['cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1af0feceb6a96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Analyze the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28cc71864cf3374d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T17:02:29.066637Z",
     "start_time": "2024-04-08T17:02:29.037752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "-1    223\n",
      " 2     49\n",
      " 5     45\n",
      " 4     17\n",
      " 0      8\n",
      " 1      8\n",
      " 3      6\n",
      "Name: count, dtype: int64\n",
      "Cluster: -1\n",
      "      user       channel                         timestamp  \\\n",
      "2     leku  #singularity  2024-04-01T02:01:10.481358+00:00   \n",
      "3    bysin  #singularity  2024-04-01T02:01:01.059385+00:00   \n",
      "6    bysin  #singularity  2024-04-01T02:00:51.586430+00:00   \n",
      "9      FE2      #!chases  2024-04-01T01:36:40.417294+00:00   \n",
      "10     FE2      #!chases  2024-04-01T01:36:35.046911+00:00   \n",
      "..     ...           ...                               ...   \n",
      "349    dio      #!chases  2024-03-31T15:49:55.416471+00:00   \n",
      "350  viral      #!chases  2024-03-31T15:49:33.444184+00:00   \n",
      "351   leku      #!chases  2024-03-31T15:49:27.437310+00:00   \n",
      "354   leku      #!chases  2024-03-31T15:49:20.825651+00:00   \n",
      "355   leku      #𝓉𝓌𝑒𝓇𝓀𝒾𝓃  2024-03-31T15:47:43.883680+00:00   \n",
      "\n",
      "                              message                  message_clean  \\\n",
      "2           i dont understand pokemon      do not understand pokemon   \n",
      "3                            last one                       last one   \n",
      "6                       watch my link                     watch link   \n",
      "9                 local trap house ??               local trap house   \n",
      "10              interactive brokers ?             interactive broker   \n",
      "..                                ...                            ...   \n",
      "349                     nah just rain                       nah rain   \n",
      "350                         bed bugs?                        bed bug   \n",
      "351                        dio: snow?                       dio snow   \n",
      "354                        bugs fixed                        bug fix   \n",
      "355  chasebot: whats going onasdfadsf  chasebot what s go onasdfadsf   \n",
      "\n",
      "                                  tokens  cluster  \n",
      "2         [do, not, understand, pokemon]       -1  \n",
      "3                            [last, one]       -1  \n",
      "6                          [watch, link]       -1  \n",
      "9                   [local, trap, house]       -1  \n",
      "10                 [interactive, broker]       -1  \n",
      "..                                   ...      ...  \n",
      "349                          [nah, rain]       -1  \n",
      "350                           [bed, bug]       -1  \n",
      "351                          [dio, snow]       -1  \n",
      "354                           [bug, fix]       -1  \n",
      "355  [chasebot, what, s, go, onasdfadsf]       -1  \n",
      "\n",
      "[223 rows x 7 columns]\n",
      "Cluster: 0\n",
      "         user   channel                         timestamp  \\\n",
      "34       leku  #!chases  2024-04-01T01:34:41.880893+00:00   \n",
      "35       leku  #!chases  2024-04-01T01:34:30.653665+00:00   \n",
      "52       leku  #!chases  2024-04-01T00:36:35.372251+00:00   \n",
      "56        nem  #!chases  2024-04-01T00:25:15.301962+00:00   \n",
      "172      leku  #!chases  2024-03-31T18:34:07.417986+00:00   \n",
      "186  larsinio  #!chases  2024-03-31T18:02:21.390198+00:00   \n",
      "187  larsinio  #!chases  2024-03-31T18:02:17.574258+00:00   \n",
      "197      leku  #!chases  2024-03-31T17:39:24.729678+00:00   \n",
      "\n",
      "                                               message  \\\n",
      "34        FE2: i found a few places to buy $DUSK again   \n",
      "35                                         `coins dusk   \n",
      "52                                         `coins dusk   \n",
      "56   I can't believe you're trying to ruin trans aw...   \n",
      "172                                        `coins dusk   \n",
      "186                                        `coins doge   \n",
      "187                                         `coins dot   \n",
      "197                                        `coins dusk   \n",
      "\n",
      "                                         message_clean  \\\n",
      "34                             fe2 find place buy dusk   \n",
      "35                                           coin dusk   \n",
      "52                                           coin dusk   \n",
      "56   can not believe you re try ruin trans awarenes...   \n",
      "172                                          coin dusk   \n",
      "186                                          coin doge   \n",
      "187                                           coin dot   \n",
      "197                                          coin dusk   \n",
      "\n",
      "                                                tokens  cluster  \n",
      "34                       [fe2, find, place, buy, dusk]        0  \n",
      "35                                        [coin, dusk]        0  \n",
      "52                                        [coin, dusk]        0  \n",
      "56   [can, not, believe, you, re, try, ruin, trans,...        0  \n",
      "172                                       [coin, dusk]        0  \n",
      "186                                       [coin, doge]        0  \n",
      "187                                        [coin, dot]        0  \n",
      "197                                       [coin, dusk]        0  \n",
      "Cluster: 1\n",
      "        user       channel                         timestamp  \\\n",
      "130      sig  #singularity  2024-03-31T19:28:26.193364+00:00   \n",
      "131      sig  #singularity  2024-03-31T19:28:26.106675+00:00   \n",
      "133      sig  #singularity  2024-03-31T19:27:39.855604+00:00   \n",
      "134      sig  #singularity  2024-03-31T19:27:39.768803+00:00   \n",
      "138      sig  #singularity  2024-03-31T19:26:53.637621+00:00   \n",
      "139   snompy  #singularity  2024-03-31T19:26:50.684278+00:00   \n",
      "165   snompy  #singularity  2024-03-31T19:08:58.985718+00:00   \n",
      "205  Raccoon      #!chases  2024-03-31T17:24:07.032080+00:00   \n",
      "\n",
      "                                               message  \\\n",
      "130  islands or Caribbean Netherlands): These are s...   \n",
      "131  besides the mainland netherlands, the kingdom ...   \n",
      "133                      netherlands in terms of size.   \n",
      "134  my comparison was based on the mainland nether...   \n",
      "138  maryland is pretty close in size to the nether...   \n",
      "139  sig: what state in usa is comparable in size w...   \n",
      "165  and ye holland is only a small region of the c...   \n",
      "205                            sig: about 258 i figure   \n",
      "\n",
      "                                         message_clean  \\\n",
      "130  islands caribbean netherlands special municipa...   \n",
      "131  besides mainland netherlands kingdom netherlan...   \n",
      "133                              netherlands term size   \n",
      "134  comparison base mainland netherlands alone inc...   \n",
      "138  maryland pretty close size netherlands netherl...   \n",
      "139           sig state usa comparable size netherland   \n",
      "165                    ye holland small region country   \n",
      "205                                     sig 258 figure   \n",
      "\n",
      "                                                tokens  cluster  \n",
      "130  [islands, caribbean, netherlands, special, mun...        1  \n",
      "131  [besides, mainland, netherlands, kingdom, neth...        1  \n",
      "133                          [netherlands, term, size]        1  \n",
      "134  [comparison, base, mainland, netherlands, alon...        1  \n",
      "138  [maryland, pretty, close, size, netherlands, n...        1  \n",
      "139    [sig, state, usa, comparable, size, netherland]        1  \n",
      "165              [ye, holland, small, region, country]        1  \n",
      "205                                 [sig, 258, figure]        1  \n",
      "Cluster: 2\n",
      "         user       channel                         timestamp  \\\n",
      "0        leku      #!chases  2024-04-01T02:21:46.712646+00:00   \n",
      "1    larsinio      #!chases  2024-04-01T02:16:12.510311+00:00   \n",
      "4        leku  #singularity  2024-04-01T02:00:56.648500+00:00   \n",
      "5        leku  #singularity  2024-04-01T02:00:53.640785+00:00   \n",
      "7       bysin  #singularity  2024-04-01T02:00:46.932146+00:00   \n",
      "8        leku  #singularity  2024-04-01T02:00:14.449805+00:00   \n",
      "16        FE2      #!chases  2024-04-01T01:35:49.151921+00:00   \n",
      "17        FE2      #!chases  2024-04-01T01:35:47.986634+00:00   \n",
      "21       leku      #!chases  2024-04-01T01:35:41.488222+00:00   \n",
      "23        FE2      #!chases  2024-04-01T01:35:30.409574+00:00   \n",
      "25        FE2      #!chases  2024-04-01T01:35:25.353221+00:00   \n",
      "30       leku      #!chases  2024-04-01T01:35:04.044568+00:00   \n",
      "32        FE2      #!chases  2024-04-01T01:34:54.539264+00:00   \n",
      "69        FE2      #!chases  2024-03-31T23:38:19.364235+00:00   \n",
      "70      viral      #!chases  2024-03-31T23:21:29.114030+00:00   \n",
      "80      viral      #!chases  2024-03-31T23:02:42.999286+00:00   \n",
      "81      viral      #!chases  2024-03-31T23:02:38.743297+00:00   \n",
      "85       leku      #!chases  2024-03-31T23:02:25.784640+00:00   \n",
      "88      viral      #!chases  2024-03-31T23:01:56.111496+00:00   \n",
      "91      viral      #!chases  2024-03-31T23:00:21.838578+00:00   \n",
      "96       leku      #!chases  2024-03-31T23:00:01.956131+00:00   \n",
      "105     viral      #!chases  2024-03-31T22:58:27.076209+00:00   \n",
      "107      leku      #!chases  2024-03-31T22:58:17.725940+00:00   \n",
      "109     viral      #!chases  2024-03-31T22:58:04.404635+00:00   \n",
      "115      leku      #!chases  2024-03-31T22:56:50.038564+00:00   \n",
      "116     viral      #!chases  2024-03-31T22:56:47.930985+00:00   \n",
      "120      leku      #!chases  2024-03-31T22:55:30.387658+00:00   \n",
      "154     farmr  #singularity  2024-03-31T19:20:14.207319+00:00   \n",
      "163     farmr      #!chases  2024-03-31T19:11:21.025985+00:00   \n",
      "182     farmr      #!chases  2024-03-31T18:09:10.528562+00:00   \n",
      "200      leku      #!chases  2024-03-31T17:36:27.200921+00:00   \n",
      "285     viral      #!chases  2024-03-31T16:52:14.944011+00:00   \n",
      "295     viral      #!chases  2024-03-31T16:45:30.951826+00:00   \n",
      "300      leku      #!chases  2024-03-31T16:43:08.030919+00:00   \n",
      "304   Hyralak      #!chases  2024-03-31T16:42:47.883474+00:00   \n",
      "307     viral      #!chases  2024-03-31T16:41:49.210255+00:00   \n",
      "310     viral      #!chases  2024-03-31T16:41:22.718307+00:00   \n",
      "317      leku      #!chases  2024-03-31T16:34:49.416597+00:00   \n",
      "332      leku      #!chases  2024-03-31T15:53:52.398315+00:00   \n",
      "334      leku      #!chases  2024-03-31T15:53:39.917771+00:00   \n",
      "335     viral      #!chases  2024-03-31T15:53:00.099236+00:00   \n",
      "337     viral      #!chases  2024-03-31T15:52:50.896403+00:00   \n",
      "339      leku      #!chases  2024-03-31T15:52:28.373679+00:00   \n",
      "341     viral      #!chases  2024-03-31T15:51:54.770079+00:00   \n",
      "344     viral      #!chases  2024-03-31T15:51:31.220140+00:00   \n",
      "345      leku      #!chases  2024-03-31T15:51:21.276375+00:00   \n",
      "346     viral      #!chases  2024-03-31T15:51:16.062591+00:00   \n",
      "347      leku      #!chases  2024-03-31T15:51:07.581312+00:00   \n",
      "352      leku      #!chases  2024-03-31T15:49:23.975034+00:00   \n",
      "\n",
      "                                      message  \\\n",
      "0                                         re?   \n",
      "1                                         sup   \n",
      "4                                   which one   \n",
      "5                                 hello there   \n",
      "7                                          hi   \n",
      "8                                          re   \n",
      "16                               good to know   \n",
      "17                                       o ok   \n",
      "21                                         no   \n",
      "23                                 u all in ?   \n",
      "25                                         oo   \n",
      "30                                         no   \n",
      "32                                       nice   \n",
      "69   do you have it with your cereal, viral ?   \n",
      "70                                        lol   \n",
      "80                               then im good   \n",
      "81                                         kk   \n",
      "85                                       same   \n",
      "88                                         ok   \n",
      "91                                      yeah,   \n",
      "96    i dont know if thats good or bad viral    \n",
      "105                                       yes   \n",
      "107                                      same   \n",
      "109                                      lolz   \n",
      "115                                      dang   \n",
      "116                                        ;o   \n",
      "120                  viral: whats for dinner?   \n",
      "154                                    i see!   \n",
      "163                                      YES!   \n",
      "182                                      nem*   \n",
      "200                                 lol farmr   \n",
      "285       what do hipsters eat for breakfast?   \n",
      "295                                       lol   \n",
      "300                                     hmm..   \n",
      "304                                       lol   \n",
      "307                                       awh   \n",
      "310                                       lol   \n",
      "317                                         k   \n",
      "332               viral: whats for breakfast?   \n",
      "334                                      dang   \n",
      "335                                        :D   \n",
      "337                                      lolz   \n",
      "339                                        u?   \n",
      "341                                     i see   \n",
      "344                                      cool   \n",
      "345                                         k   \n",
      "346                                         k   \n",
      "347                      viral: not this time   \n",
      "352                                viral: sup   \n",
      "\n",
      "                         message_clean  \\\n",
      "0                                        \n",
      "1                                  sup   \n",
      "4                                  one   \n",
      "5                                hello   \n",
      "7                                   hi   \n",
      "8                                        \n",
      "16                           good know   \n",
      "17                                  ok   \n",
      "21                                       \n",
      "23                                   u   \n",
      "25                                  oo   \n",
      "30                                       \n",
      "32                                nice   \n",
      "69                        cereal viral   \n",
      "70                                 lol   \n",
      "80                            I m good   \n",
      "81                                  kk   \n",
      "85                                       \n",
      "88                                  ok   \n",
      "91                                yeah   \n",
      "96   do not know that s good bad viral   \n",
      "105                                yes   \n",
      "107                                      \n",
      "109                               lolz   \n",
      "115                               dang   \n",
      "116                                      \n",
      "120                viral what s dinner   \n",
      "154                                see   \n",
      "163                                yes   \n",
      "182                                nem   \n",
      "200                          lol farmr   \n",
      "285              hipster eat breakfast   \n",
      "295                                lol   \n",
      "300                                hmm   \n",
      "304                                lol   \n",
      "307                                awh   \n",
      "310                                lol   \n",
      "317                                  k   \n",
      "332             viral what s breakfast   \n",
      "334                               dang   \n",
      "335                                      \n",
      "337                               lolz   \n",
      "339                                  u   \n",
      "341                                see   \n",
      "344                               cool   \n",
      "345                                  k   \n",
      "346                                  k   \n",
      "347                         viral time   \n",
      "352                          viral sup   \n",
      "\n",
      "                                         tokens  cluster  \n",
      "0                                            []        2  \n",
      "1                                         [sup]        2  \n",
      "4                                         [one]        2  \n",
      "5                                       [hello]        2  \n",
      "7                                          [hi]        2  \n",
      "8                                            []        2  \n",
      "16                                 [good, know]        2  \n",
      "17                                         [ok]        2  \n",
      "21                                           []        2  \n",
      "23                                          [u]        2  \n",
      "25                                         [oo]        2  \n",
      "30                                           []        2  \n",
      "32                                       [nice]        2  \n",
      "69                              [cereal, viral]        2  \n",
      "70                                        [lol]        2  \n",
      "80                                 [I, m, good]        2  \n",
      "81                                         [kk]        2  \n",
      "85                                           []        2  \n",
      "88                                         [ok]        2  \n",
      "91                                       [yeah]        2  \n",
      "96   [do, not, know, that, s, good, bad, viral]        2  \n",
      "105                                       [yes]        2  \n",
      "107                                          []        2  \n",
      "109                                      [lolz]        2  \n",
      "115                                      [dang]        2  \n",
      "116                                          []        2  \n",
      "120                    [viral, what, s, dinner]        2  \n",
      "154                                       [see]        2  \n",
      "163                                       [yes]        2  \n",
      "182                                       [nem]        2  \n",
      "200                                [lol, farmr]        2  \n",
      "285                   [hipster, eat, breakfast]        2  \n",
      "295                                       [lol]        2  \n",
      "300                                       [hmm]        2  \n",
      "304                                       [lol]        2  \n",
      "307                                       [awh]        2  \n",
      "310                                       [lol]        2  \n",
      "317                                         [k]        2  \n",
      "332                 [viral, what, s, breakfast]        2  \n",
      "334                                      [dang]        2  \n",
      "335                                          []        2  \n",
      "337                                      [lolz]        2  \n",
      "339                                         [u]        2  \n",
      "341                                       [see]        2  \n",
      "344                                      [cool]        2  \n",
      "345                                         [k]        2  \n",
      "346                                         [k]        2  \n",
      "347                               [viral, time]        2  \n",
      "352                                [viral, sup]        2  \n",
      "Cluster: 3\n",
      "     user   channel                         timestamp message message_clean  \\\n",
      "36   leku  #!chases  2024-04-01T01:34:23.975083+00:00   .corn          corn   \n",
      "53   leku  #!chases  2024-04-01T00:36:32.322314+00:00   .corn          corn   \n",
      "127  leku  #!chases  2024-03-31T22:52:34.593956+00:00   .corn          corn   \n",
      "173  leku  #!chases  2024-03-31T18:34:05.088456+00:00   .corn          corn   \n",
      "198  leku  #!chases  2024-03-31T17:39:22.964224+00:00   .corn          corn   \n",
      "353  leku  #!chases  2024-03-31T15:49:22.945638+00:00   .corn          corn   \n",
      "\n",
      "     tokens  cluster  \n",
      "36   [corn]        3  \n",
      "53   [corn]        3  \n",
      "127  [corn]        3  \n",
      "173  [corn]        3  \n",
      "198  [corn]        3  \n",
      "353  [corn]        3  \n",
      "Cluster: 4\n",
      "        user   channel                         timestamp  \\\n",
      "121     leku  #!chases  2024-03-31T22:54:40.408087+00:00   \n",
      "122    viral  #!chases  2024-03-31T22:54:23.493240+00:00   \n",
      "123    viral  #!chases  2024-03-31T22:54:09.722739+00:00   \n",
      "146      sig  #!chases  2024-03-31T19:22:31.794635+00:00   \n",
      "147      sig  #!chases  2024-03-31T19:22:31.760407+00:00   \n",
      "148      nem  #!chases  2024-03-31T19:22:14.402510+00:00   \n",
      "170      sig  #!chases  2024-03-31T18:54:48.979400+00:00   \n",
      "171    viral  #!chases  2024-03-31T18:54:41.330224+00:00   \n",
      "185    farmr  #!chases  2024-03-31T18:08:12.103743+00:00   \n",
      "189    viral  #!chases  2024-03-31T17:49:14.674283+00:00   \n",
      "194     Smax  #!chases  2024-03-31T17:42:39.748255+00:00   \n",
      "222  Raccoon  #!chases  2024-03-31T17:16:22.221382+00:00   \n",
      "270  Raccoon  #!chases  2024-03-31T17:00:42.412759+00:00   \n",
      "290      nem  #!chases  2024-03-31T16:47:00.719569+00:00   \n",
      "299     Smax  #!chases  2024-03-31T16:43:41.387584+00:00   \n",
      "302    viral  #!chases  2024-03-31T16:43:01.710117+00:00   \n",
      "330    viral  #!chases  2024-03-31T15:54:30.153523+00:00   \n",
      "\n",
      "                                               message  \\\n",
      "121                                      leKUL moe dee   \n",
      "122                                       leku moe dee   \n",
      "123                                        leku is MOe   \n",
      "146  the same day as trans visibility awareness day...   \n",
      "147  alright, imagine this:  in a small, vibrant to...   \n",
      "148  sig: tell us a trans visibility awareness day ...   \n",
      "170  once upon an easter sunday in a small, quirky ...   \n",
      "171                       sig: tell us an Easter story   \n",
      "185                                               leku   \n",
      "189                          leku living his best life   \n",
      "194  leku    What are you doing today to celebrate ...   \n",
      "222                                          sig: lol.   \n",
      "270                              sig: but did it work?   \n",
      "290  leku said he was going down to the ol' gender ...   \n",
      "299  leku    What are you doing today to celebrate ...   \n",
      "302             i think it’s the other way around leku   \n",
      "330                                    idk, leku, you?   \n",
      "\n",
      "                                         message_clean  \\\n",
      "121                                      lekul moe dee   \n",
      "122                                       leku moe dee   \n",
      "123                                           leku moe   \n",
      "146  day trans visibility awareness day ellie idea ...   \n",
      "147  alright imagine small vibrant town know tightk...   \n",
      "148  sig tell we trans visibility awareness day sto...   \n",
      "170  upon easter sunday small quirky town notorious...   \n",
      "171                           sig tell we easter story   \n",
      "185                                               leku   \n",
      "189                                leku live good life   \n",
      "194   leku today celebrate bidens trans visibility day   \n",
      "222                                            sig lol   \n",
      "270                                           sig work   \n",
      "290         leku say go ol gender clinic get dick chop   \n",
      "299   leku today celebrate bidens trans visibility day   \n",
      "302                              think way around leku   \n",
      "330                                           idk leku   \n",
      "\n",
      "                                                tokens  cluster  \n",
      "121                                  [lekul, moe, dee]        4  \n",
      "122                                   [leku, moe, dee]        4  \n",
      "123                                        [leku, moe]        4  \n",
      "146  [day, trans, visibility, awareness, day, ellie...        4  \n",
      "147  [alright, imagine, small, vibrant, town, know,...        4  \n",
      "148  [sig, tell, we, trans, visibility, awareness, ...        4  \n",
      "170  [upon, easter, sunday, small, quirky, town, no...        4  \n",
      "171                     [sig, tell, we, easter, story]        4  \n",
      "185                                             [leku]        4  \n",
      "189                           [leku, live, good, life]        4  \n",
      "194  [leku, today, celebrate, bidens, trans, visibi...        4  \n",
      "222                                         [sig, lol]        4  \n",
      "270                                        [sig, work]        4  \n",
      "290  [leku, say, go, ol, gender, clinic, get, dick,...        4  \n",
      "299  [leku, today, celebrate, bidens, trans, visibi...        4  \n",
      "302                         [think, way, around, leku]        4  \n",
      "330                                        [idk, leku]        4  \n",
      "Cluster: 5\n",
      "        user   channel                         timestamp  \\\n",
      "41       sig  #!chases  2024-04-01T00:43:12.995560+00:00   \n",
      "42       sig  #!chases  2024-04-01T00:43:12.908813+00:00   \n",
      "44       sig  #!chases  2024-04-01T00:40:08.853611+00:00   \n",
      "45       sig  #!chases  2024-04-01T00:40:08.849693+00:00   \n",
      "46       sig  #!chases  2024-04-01T00:40:08.815502+00:00   \n",
      "48       sig  #!chases  2024-04-01T00:38:37.356824+00:00   \n",
      "49       sig  #!chases  2024-04-01T00:38:37.353993+00:00   \n",
      "51      leku  #!chases  2024-04-01T00:38:30.859962+00:00   \n",
      "60       sig  #!chases  2024-03-31T23:58:34.835486+00:00   \n",
      "61       sig  #!chases  2024-03-31T23:58:34.748581+00:00   \n",
      "62       td-  #!chases  2024-03-31T23:58:29.420306+00:00   \n",
      "64       sig  #!chases  2024-03-31T23:57:31.286781+00:00   \n",
      "215      sig  #!chases  2024-03-31T17:18:39.244203+00:00   \n",
      "216      sig  #!chases  2024-03-31T17:18:39.157456+00:00   \n",
      "217  Raccoon  #!chases  2024-03-31T17:18:32.224381+00:00   \n",
      "219      sig  #!chases  2024-03-31T17:17:38.771391+00:00   \n",
      "224      sig  #!chases  2024-03-31T17:16:10.196447+00:00   \n",
      "225  Raccoon  #!chases  2024-03-31T17:16:06.477030+00:00   \n",
      "228      sig  #!chases  2024-03-31T17:13:52.382516+00:00   \n",
      "229  Raccoon  #!chases  2024-03-31T17:13:47.213174+00:00   \n",
      "230      sig  #!chases  2024-03-31T17:08:44.178123+00:00   \n",
      "231      sig  #!chases  2024-03-31T17:08:44.144059+00:00   \n",
      "234      sig  #!chases  2024-03-31T17:08:16.136869+00:00   \n",
      "235  Raccoon  #!chases  2024-03-31T17:08:13.089396+00:00   \n",
      "236      sig  #!chases  2024-03-31T17:07:27.979049+00:00   \n",
      "237      sig  #!chases  2024-03-31T17:07:27.944807+00:00   \n",
      "238  Raccoon  #!chases  2024-03-31T17:07:22.714126+00:00   \n",
      "239      sig  #!chases  2024-03-31T17:06:33.429016+00:00   \n",
      "242      sig  #!chases  2024-03-31T17:06:08.249325+00:00   \n",
      "243  Raccoon  #!chases  2024-03-31T17:06:05.480758+00:00   \n",
      "245  Raccoon  #!chases  2024-03-31T17:05:13.451333+00:00   \n",
      "249      sig  #!chases  2024-03-31T17:04:18.562939+00:00   \n",
      "250      sig  #!chases  2024-03-31T17:04:18.475980+00:00   \n",
      "252      sig  #!chases  2024-03-31T17:03:50.557132+00:00   \n",
      "255      sig  #!chases  2024-03-31T17:03:14.846160+00:00   \n",
      "256      sig  #!chases  2024-03-31T17:03:14.811690+00:00   \n",
      "257      sig  #!chases  2024-03-31T17:03:10.511061+00:00   \n",
      "258      sig  #!chases  2024-03-31T17:03:10.476879+00:00   \n",
      "260    eefer  #!chases  2024-03-31T17:03:06.363845+00:00   \n",
      "264      sig  #!chases  2024-03-31T17:01:56.863513+00:00   \n",
      "265      sig  #!chases  2024-03-31T17:01:56.829153+00:00   \n",
      "266    eefer  #!chases  2024-03-31T17:01:51.033409+00:00   \n",
      "267      sig  #!chases  2024-03-31T17:01:34.166377+00:00   \n",
      "279  Raccoon  #!chases  2024-03-31T16:57:33.967620+00:00   \n",
      "296  Raccoon  #!chases  2024-03-31T16:45:04.797505+00:00   \n",
      "\n",
      "                                               message  \\\n",
      "41   on these important issues. understanding our h...   \n",
      "42   fair point, and i get where you're coming from...   \n",
      "44   went into building these monuments and appreci...   \n",
      "45   they were created, but rather an appreciation ...   \n",
      "46   you're hitting on a really complex issue here....   \n",
      "48   aspects of human civilization—how people lived...   \n",
      "49   another level. they're not just old buildings;...   \n",
      "51   sig: how come we dont take down all of the pyr...   \n",
      "60   as a trendy costume without understanding its ...   \n",
      "61   oh, it's definitely not as simple as just expl...   \n",
      "62   sig: so is cultural appropriation just people ...   \n",
      "64   ah, the never-ending debate about cultural app...   \n",
      "215  contribute to society and peace-making, regard...   \n",
      "216  that's a fundamentally flawed and racist persp...   \n",
      "217  sig: But you're still correct that blacks do n...   \n",
      "219  that's not true. criticisms of IQ tests span v...   \n",
      "224  nope, that's not just wrong; it's dangerous mi...   \n",
      "225  sig: the average IQ of all people tested for a...   \n",
      "228  that's a load of racist nonsense. intelligence...   \n",
      "229  sig: you forget that slave owners were dealing...   \n",
      "230  between various forms of economic or social pr...   \n",
      "231  throwing around numbers like \"93%\" without con...   \n",
      "234  if you're looking for a debate that twists his...   \n",
      "235  sig: being dismissive of your invalid presuppo...   \n",
      "236  descendants. the struggle for rights, freedom,...   \n",
      "237  that's blatantly incorrect and dismissive of t...   \n",
      "238  sig: black americans were never happier nor mo...   \n",
      "239                    coerced labor and exploitation.   \n",
      "242  that's a pretty rough take on a complex and br...   \n",
      "243  sig: all slavery is just fostering another hun...   \n",
      "245  sig: you are a woke bot who enjoys conflaiting...   \n",
      "249  procedures, on the other hand, involve informe...   \n",
      "250  not at all, what we're talking about are issue...   \n",
      "252  crucial to understand that the context and rea...   \n",
      "255  performed in a completely different context, w...   \n",
      "256  alright, let's not conflate wildly different c...   \n",
      "257  the public eye doesn't mean it was any less ho...   \n",
      "258  it's not about getting a \"free pass.\" the diff...   \n",
      "260  sig: It sounds like the middle easterners were...   \n",
      "264  emphasized, and frankly, there's a lot of hist...   \n",
      "265  yeah, the focus on the transatlantic slave tra...   \n",
      "266  sig: Why is this not more well known? All talk...   \n",
      "267  well, that's a pretty cold take. history’s ful...   \n",
      "279  This serves the dual purpose of making sure ne...   \n",
      "296  only reason whites are supreme is because we s...   \n",
      "\n",
      "                                         message_clean  \\\n",
      "41   important issue understand history good bad es...   \n",
      "42   fair point get you re come discussion around h...   \n",
      "44   go build monument appreciate contribution huma...   \n",
      "45   create rather appreciation historical architec...   \n",
      "46   you re hit really complex issue true construct...   \n",
      "48   aspect human civilizationhow people live work ...   \n",
      "49   another level they re old building they re par...   \n",
      "51   sig come do not take pyramid ancient site buil...   \n",
      "60   trendy costume without understand significance...   \n",
      "61   oh definitely simple explore range cultural ap...   \n",
      "62   sig cultural appropriation people explore rang...   \n",
      "64   ah neverending debate cultural appropriation h...   \n",
      "215  contribute society peacemake regardless race r...   \n",
      "216  that s fundamentally flawed racist perspective...   \n",
      "217  sig you re still correct black fit white socie...   \n",
      "219  that s true criticism iq test span various gro...   \n",
      "224  nope that s wrong dangerous misinformation iq ...   \n",
      "225  sig average iq people test aptitude african co...   \n",
      "228  that s load racist nonsense intelligence behav...   \n",
      "229  sig forget slave owner deal extremely low inte...   \n",
      "230  various form economic social pressure outright...   \n",
      "231  throw around number like 93 without context do...   \n",
      "234  you re look debate twist historical cruelty so...   \n",
      "235  sig dismissive invalid presupposition argue sl...   \n",
      "236  descendant struggle right freedom recognition ...   \n",
      "237  that s blatantly incorrect dismissive severe t...   \n",
      "238  sig black americans never happy complete slave...   \n",
      "239                          coerce labor exploitation   \n",
      "242  that s pretty rough take complex brutal part h...   \n",
      "243  sig slavery foster another hungry mouth someon...   \n",
      "245  sig woke bot enjoy conflaite bullshit argument...   \n",
      "249  procedure hand involve inform consent aim alig...   \n",
      "250  talk issue consent context historical signific...   \n",
      "252  crucial understand context reason medical proc...   \n",
      "255  perform completely different context consent m...   \n",
      "256  alright let conflate wildly different context ...   \n",
      "257  public eye do not mean less horrific form slav...   \n",
      "258  get free pass difference legacy public conscio...   \n",
      "260  sig sound like middle easterner something thin...   \n",
      "264  emphasize frankly there s lot history get glos...   \n",
      "265  yeah focus transatlantic slave trade lot massi...   \n",
      "266  sig well know talk slavery transatlantic slave...   \n",
      "267  well that s pretty cold take history full succ...   \n",
      "279  serve dual purpose make sure negro could not r...   \n",
      "296  reason white supreme subject thousand year pub...   \n",
      "\n",
      "                                                tokens  cluster  \n",
      "41   [important, issue, understand, history, good, ...        5  \n",
      "42   [fair, point, get, you, re, come, discussion, ...        5  \n",
      "44   [go, build, monument, appreciate, contribution...        5  \n",
      "45   [create, rather, appreciation, historical, arc...        5  \n",
      "46   [you, re, hit, really, complex, issue, true, c...        5  \n",
      "48   [aspect, human, civilizationhow, people, live,...        5  \n",
      "49   [another, level, they, re, old, building, they...        5  \n",
      "51   [sig, come, do, not, take, pyramid, ancient, s...        5  \n",
      "60   [trendy, costume, without, understand, signifi...        5  \n",
      "61   [oh, definitely, simple, explore, range, cultu...        5  \n",
      "62   [sig, cultural, appropriation, people, explore...        5  \n",
      "64   [ah, neverending, debate, cultural, appropriat...        5  \n",
      "215  [contribute, society, peacemake, regardless, r...        5  \n",
      "216  [that, s, fundamentally, flawed, racist, persp...        5  \n",
      "217  [sig, you, re, still, correct, black, fit, whi...        5  \n",
      "219  [that, s, true, criticism, iq, test, span, var...        5  \n",
      "224  [nope, that, s, wrong, dangerous, misinformati...        5  \n",
      "225  [sig, average, iq, people, test, aptitude, afr...        5  \n",
      "228  [that, s, load, racist, nonsense, intelligence...        5  \n",
      "229  [sig, forget, slave, owner, deal, extremely, l...        5  \n",
      "230  [various, form, economic, social, pressure, ou...        5  \n",
      "231  [throw, around, number, like, 93, without, con...        5  \n",
      "234  [you, re, look, debate, twist, historical, cru...        5  \n",
      "235  [sig, dismissive, invalid, presupposition, arg...        5  \n",
      "236  [descendant, struggle, right, freedom, recogni...        5  \n",
      "237  [that, s, blatantly, incorrect, dismissive, se...        5  \n",
      "238  [sig, black, americans, never, happy, complete...        5  \n",
      "239                      [coerce, labor, exploitation]        5  \n",
      "242  [that, s, pretty, rough, take, complex, brutal...        5  \n",
      "243  [sig, slavery, foster, another, hungry, mouth,...        5  \n",
      "245  [sig, woke, bot, enjoy, conflaite, bullshit, a...        5  \n",
      "249  [procedure, hand, involve, inform, consent, ai...        5  \n",
      "250  [talk, issue, consent, context, historical, si...        5  \n",
      "252  [crucial, understand, context, reason, medical...        5  \n",
      "255  [perform, completely, different, context, cons...        5  \n",
      "256  [alright, let, conflate, wildly, different, co...        5  \n",
      "257  [public, eye, do, not, mean, less, horrific, f...        5  \n",
      "258  [get, free, pass, difference, legacy, public, ...        5  \n",
      "260  [sig, sound, like, middle, easterner, somethin...        5  \n",
      "264  [emphasize, frankly, there, s, lot, history, g...        5  \n",
      "265  [yeah, focus, transatlantic, slave, trade, lot...        5  \n",
      "266  [sig, well, know, talk, slavery, transatlantic...        5  \n",
      "267  [well, that, s, pretty, cold, take, history, f...        5  \n",
      "279  [serve, dual, purpose, make, sure, negro, coul...        5  \n",
      "296  [reason, white, supreme, subject, thousand, ye...        5  \n"
     ]
    }
   ],
   "source": [
    "# Explore the number of messages per cluster\n",
    "print(chat_logs_df['cluster'].value_counts())\n",
    "\n",
    "# Inspect a specific cluster\n",
    "#print(chat_logs_df[chat_logs_df['cluster'] == 0])\n",
    "\n",
    "# Group the DataFrame by the 'cluster' column\n",
    "grouped_df = chat_logs_df.groupby('cluster')\n",
    "\n",
    "# Iterate through each group\n",
    "for cluster_label, group in grouped_df:\n",
    "    print(f\"Cluster: {cluster_label}\")\n",
    "    print(group)  # 'group' is a DataFrame containing only the rows from this cluster\n",
    "    # You can perform further analysis or processing on each group here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "387b8cf1ffff4919",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
